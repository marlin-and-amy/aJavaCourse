## MidJava 3-4 Reaction Architechture [&LT;](MJ0303.md) [&GT;](MJ0305.md)

# Shape Recognition

In this system, a gesture is what happens during a single mouse drag. On mouse down we start capturing mouse coordinates, we continue capturing them as the drag happens, and we get our last point when the mouse goes up.

That list of coordinates is a single Polyline, **PL**. We take those coordinates and subsample them to a fixed number of coordinates (I typically use 25 xy pairs) either by throwing out points if there were too many or by duplicating points if there were too few. We do not interpolate between points. 

Next we scale and translate those points to a uniform coordinate system (we used 0 to 1000). This is called **Normalizing** the PL. The scaling is done isomorphically (meaning that we scale both x and y by the same value).

Once we have sub-sampled and normalized the input, we can compare the input to a list of **prototypes** (which are also just sub-sampled and normalized input - though we may have blended together several inputs in order to smooth a prototype by averaging). Each prototype is owned by a **Shape**. 

So for example, one of the Shapes could be a "Circle". You may have several different ways of drawing a circle (for example you could start at the top and go clockwise, start at the bottom and go counter-clockwise, lots of possibilities!) and thus the circle Shape, which is a virtual abstract concept sort of notion, may have several different prototypes that represent specific ways (typically different ways) of drawing that particular shape.

When you want to match an input to a prototype, you just compute the Euclidean distance (a metric function) between the two, i.e. root sum of squared differences. (Note, because we only use the distance for comparison - is this one bigger than that one - we don't actually need to take a square root because root(A) < root(B) only if A < B so it is sufficient to compare the squared distance rather than the actual distance.) The bigger the distance the less the one polyline matches the other.

The matching that we are doing is known as "Nearest Neighbor Matching" You have a list of ALL prototypes, each of which is classified as being of a given Shape. You find the prototype that is "closest" (in Euclidean distance) to the input shape, and that would be the input's nearest neighbor in prototype space, and you DECLARE that the input should be classified as being the same Shape as that matched prototype's Shape. (Note: common variants of Nearest Neighbor include things like finding the 3 nearest neighbors and letting them vote: i.e majority determine the shape. We don't bother. We just use simple nearest neighbor matching)

So the flow of information through the java classes when we are recognizing gestures looks like this:

1. Ink.BUFFER - capture all xy coordinates from one mouse down to a mouse up.
2. Ink.BUFFER.bbox - compute the bounding box of the ink in the Ink.BUFFER
2. Ink.BUFFER.norm - sub-sample the ink in the buffer down to 25 points and normalize them
2. Go through all the Shapes in the Shape Database find the closest match.
2. Now that you have the recognized shape, create a Gesture(a Stroke) which is a Shape AND the VS where it happened.
2. Reactions are classified (kept in lists) by the Shape that they are looking for. So there is a Reactions.Map<Shape, ReactionList>
2. go through the ReactionList and find the winning bidder.
2. call the act() function for the winning bidder.
2. finally, place the Stroke on the Undo stack (so that you can undo/redo that gesture if necessary)


In outline Here are some of the reaction classes, the ones involved with shape recognition, and their memebers

0. **Ink** extends **Norm** - vs  // We normalize our ink but remember where it happend on the screen.
2. **Ink.Buffer** extends **PL** - n, bbox  // we keep an array of points, n is how many, and bbox is the bounds
2. **Norm** extends **PL**    // a single polyline in the normalized coordinate system
2. **Norm.List** extends **ArrayList&lt;Norm&gt;**
2. **Shape** extends **ArrayList&lt;Prototype&gt;** - name  // a name like "N-N" and all the ways of drawing that shape 
2. **Shape.Prototype** extends **Norm** - nBlend // a single way of drawing a shape, nBlend is the count of averaging
2. **Stroke** extends **VS** - shape  // a Stoke is WHERE a shape happened on screen


You do not need to create any of these classes yet. I am just outlining what we will be doing and giving you vocabulary, names for the components that make up the shape recognition process.

[next: Compass Gestures](MJ0305.md)
